{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# With optuna"
      ],
      "metadata": {
        "id": "h71g3awpDO3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "python\n",
        "\n",
        "\n",
        "import optuna\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, AdamW, get_scheduler\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# 1. Load Twitter Dataset\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/PratishMashankar/twitter-sentiment-analysis/refs/heads/master/data/Twitter_Data.csv\")\n",
        "\n",
        "# Map Sentiments to Numeric\n",
        "df['category'] = df['category'].map({'Positive': 1, 'Negative': 0})\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['clean_text'].tolist(),\n",
        "    df['category'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# 2. Train Model Function\n",
        "def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, device, trial_number):\n",
        "    best_val_loss = np.inf\n",
        "    patience_counter = 0\n",
        "    patience = 3\n",
        "    num_epochs = 5\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            labels = batch.pop('label')\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**batch)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                labels = batch.pop('label')\n",
        "                outputs = model(**batch)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        mlflow.log_metric(\"val_loss\", avg_val_loss, step=epoch)\n",
        "\n",
        "        # Early stopping\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), f\"best_model_trial{trial_number}.pth\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "# 3. Log Parameters\n",
        "def log_params(trial):\n",
        "    params = {\n",
        "        \"learning_rate\": trial.params[\"learning_rate\"],\n",
        "        \"batch_size\": trial.params[\"batch_size\"]\n",
        "    }\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "# 4. Evaluate Model\n",
        "def evaluate_model(model, dataloader):\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            labels = batch.pop('label')\n",
        "            outputs = model(**batch)\n",
        "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    report = classification_report(all_labels, all_preds, output_dict=True)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, report, conf_matrix\n",
        "\n",
        "# 5. Log Metrics and Artifacts\n",
        "def log_metrics_artifacts(accuracy, report, conf_matrix, trial_number, model):\n",
        "    mlflow.log_metric(\"test_accuracy\", accuracy)\n",
        "\n",
        "    for label, metrics in report.items():\n",
        "        if isinstance(metrics, dict):\n",
        "            for metric_name, metric_value in metrics.items():\n",
        "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    conf_matrix_path = f\"confusion_matrix_trial{trial_number}.png\"\n",
        "    plt.savefig(conf_matrix_path)\n",
        "    mlflow.log_artifact(conf_matrix_path)\n",
        "\n",
        "    mlflow.pytorch.log_model(model, \"models/distilbert_model\")\n",
        "\n",
        "# 6. Objective Function\n",
        "def objective(trial):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
        "\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2).to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    train_dataset = TwitterDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = TwitterDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    num_training_steps = len(train_loader) * 5\n",
        "    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    mlflow.set_experiment(\"DistilBERT_Twitter_Optuna\")\n",
        "    with mlflow.start_run(run_name=f\"Trial_{trial.number}\"):\n",
        "        log_params(trial)\n",
        "\n",
        "        model = train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, device, trial.number)\n",
        "        model.load_state_dict(torch.load(f\"best_model_trial{trial.number}.pth\"))\n",
        "\n",
        "        accuracy, report, conf_matrix = evaluate_model(model, val_loader)\n",
        "        log_metrics_artifacts(accuracy, report, conf_matrix, trial.number, model)\n",
        "\n",
        "    return 1.0 - accuracy\n",
        "\n",
        "# 7. Run Optuna Study\n",
        "def run_optuna_study(n_trials=10):\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_accuracy = 1.0 - study.best_value\n",
        "\n",
        "    print(\"Best hyperparameters:\", best_params)\n",
        "    print(\"Best accuracy:\", best_accuracy)\n",
        "\n",
        "    return study\n",
        "\n",
        "# 8. Execute\n",
        "best_study = run_optuna_study(n_trials=10)'\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "BIESWjgeFRBS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82y5bWk-BojM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Without optuna"
      ],
      "metadata": {
        "id": "riB4zHkxDU1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers mlflow dagshub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6ZofGka5DvyL",
        "outputId": "af4c6712-e394-4a5c-ca08-409b572854eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.11/dist-packages (2.21.3)\n",
            "Requirement already satisfied: dagshub in /usr/local/lib/python3.11/dist-packages (0.5.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.2.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: mlflow-skinny==2.21.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.21.3)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.2)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.11/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.14.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.40)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (3.1.1)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (0.50.0)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (0.115.12)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (8.6.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (1.31.1)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (2.11.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (0.5.3)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (0.34.1)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.4.4)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (13.9.4)\n",
            "Requirement already satisfied: dacite~=1.6.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.6.0)\n",
            "Requirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (9.1.2)\n",
            "Requirement already satisfied: gql[requests] in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.5.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.6.7)\n",
            "Requirement already satisfied: treelib>=1.6.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.7.1)\n",
            "Requirement already satisfied: pathvalidate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.8.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.37.36)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.0.4)\n",
            "Requirement already satisfied: dagshub-annotation-converter>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.1.9)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.3.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.1.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.21.3->mlflow) (4.0.12)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.4)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->dagshub) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.18.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: botocore<1.38.0,>=1.37.36 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.37.36)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (0.11.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (0.9.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.19.0)\n",
            "Requirement already satisfied: backoff<3.0,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (2.2.1)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.3->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==2.21.3->mlflow) (0.46.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.21.3->mlflow) (5.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.21.3->mlflow) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.21.3->mlflow) (1.2.18)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.21.3->mlflow) (0.52b1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.21.3->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.21.3->mlflow) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.21.3->mlflow) (0.4.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub) (1.0.0)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.21.3->mlflow) (1.17.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.3->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.3->mlflow) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.3->mlflow) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import dagshub\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, get_scheduler\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "import mlflow\n",
        "import dagshub\n",
        "import json\n",
        "\n",
        "# Load JSON from file\n",
        "with open('/content/youtube_dagshub.json', 'r', encoding='utf-8') as f:\n",
        "    token = json.load(f)\n",
        "\n",
        "DAGSHUB_USER=\"Prayesh13\"\n",
        "DAGSHUB_REPO=\"youtube-comments-analysis\"\n",
        "\n",
        "\n",
        "mlflow.set_tracking_uri(f\"https://dagshub.com/{DAGSHUB_USER}/{DAGSHUB_REPO}.mlflow\")\n",
        "dagshub.auth.add_app_token(token['youtube_dagshub'])\n",
        "dagshub.init(repo_owner='Prayesh13', repo_name='youtube-comments-analysis', mlflow=True, )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "id": "JclC6QcADXcd",
        "outputId": "b8a5297a-a71e-4a9c-e594-67907bf84efb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:dagshub.auth.tokens:The added token already exists in the token cache, skipping\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as Prayesh13\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as Prayesh13\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"Prayesh13/youtube-comments-analysis\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"Prayesh13/youtube-comments-analysis\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository Prayesh13/youtube-comments-analysis initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository Prayesh13/youtube-comments-analysis initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Twitter Dataset\n",
        "df = pd.read_csv(\"/content/sentiment_processed.csv\")\n",
        "\n",
        "# Map Sentiments to Numeric\n",
        "df['label'] = df['label'].map({-1:0, 0:1, 1:2})\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['content'].tolist(),\n",
        "    df['label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meJN4i3pD_Zg",
        "outputId": "37039dd7-5d99-4da4-bd19-e23051e8180e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# 2. Prepare DataLoaders\n",
        "train_dataset = TwitterDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = TwitterDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "kvUPyqKtEHQz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "python\n",
        "\n",
        "# 3. Initialize Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2).to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_training_steps = len(train_loader) * 5\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# 4. Training Loop\n",
        "num_epochs = 5\n",
        "best_val_loss = np.inf\n",
        "patience = 3\n",
        "patience_counter = 0\n",
        "\n",
        "mlflow.set_experiment(\"Pretrained Model\")\n",
        "with mlflow.start_run(run_name=\"distilbert\"):\n",
        "    mlflow.set_tag(\"model_type\", \"distilbert\")\n",
        "    mlflow.log_param(\"model_name\", \"distilbert-base-uncased\")\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            labels = batch.pop('label')\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**batch)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                labels = batch.pop('label')\n",
        "                outputs = model(**batch)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "                all_preds.extend(preds)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        mlflow.log_metric(\"val_loss\", avg_val_loss, step=epoch)\n",
        "        mlflow.log_metric(\"val_accuracy\", accuracy, step=epoch)\n",
        "\n",
        "        print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Early Stopping\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"best_baseline_model.pth\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "    # Final Evaluation\n",
        "    report = classification_report(all_labels, all_preds, output_dict=True)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Log Confusion Matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.savefig(\"confusion_matrix_baseline.png\")\n",
        "    mlflow.log_artifact(\"confusion_matrix_baseline.png\")\n",
        "\n",
        "    # Log Model\n",
        "    mlflow.pytorch.log_model(model, \"models/distilbert_baseline\")\n",
        "```"
      ],
      "metadata": {
        "id": "CPQJVl0rF0f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ... (Your existing code) ...\n",
        "\n",
        "# Before the training loop\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "wZHsoReJEK2v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "source": [
        "# Before loading the model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 3. Initialize Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3).to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_training_steps = len(train_loader) * 5\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# 4. Training Loop\n",
        "num_epochs = 5\n",
        "best_val_loss = np.inf\n",
        "patience = 3\n",
        "patience_counter = 0\n",
        "\n",
        "mlflow.set_experiment(\"Pretrained Model\")\n",
        "with mlflow.start_run(run_name=\"distilbert\"):\n",
        "    mlflow.set_tag(\"model_type\", \"distilbert\")\n",
        "    mlflow.log_param(\"model_name\", \"distilbert-base-uncased\")\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()} # Move batch to device\n",
        "            labels = batch.pop('label')\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**batch)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                labels = batch.pop('label')\n",
        "                outputs = model(**batch)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "                all_preds.extend(preds)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        mlflow.log_metric(\"val_loss\", avg_val_loss, step=epoch)\n",
        "        mlflow.log_metric(\"val_accuracy\", accuracy, step=epoch)\n",
        "\n",
        "        print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Early Stopping\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"best_baseline_model.pth\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "    # Final Evaluation\n",
        "    report = classification_report(all_labels, all_preds, output_dict=True)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Log Confusion Matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.savefig(\"confusion_matrix_baseline.png\")\n",
        "    mlflow.log_artifact(\"confusion_matrix_baseline.png\")\n",
        "\n",
        "    # Log Model\n",
        "    mlflow.pytorch.log_model(model, \"models/distilbert_baseline\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fsrzdndFxqI",
        "outputId": "ddc6b251-97d0-47fc-fef2-924af50bc605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# !pip install --upgrade --force-reinstall transformers"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2pO9Us8CG9R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CFVPxlHlGKdE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}