{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":815876,"sourceType":"datasetVersion","datasetId":429085}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:18:04.812220Z","iopub.execute_input":"2025-04-16T13:18:04.812971Z","iopub.status.idle":"2025-04-16T13:18:05.091907Z","shell.execute_reply.started":"2025-04-16T13:18:04.812917Z","shell.execute_reply":"2025-04-16T13:18:05.091269Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/twitter-and-reddit-sentimental-analysis-dataset/Twitter_Data.csv\n/kaggle/input/twitter-and-reddit-sentimental-analysis-dataset/Reddit_Data.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Install Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision torchaudio torchtext","metadata":{"execution":{"iopub.status.busy":"2025-04-16T13:18:05.093151Z","iopub.execute_input":"2025-04-16T13:18:05.093460Z","iopub.status.idle":"2025-04-16T13:21:03.943624Z","shell.execute_reply.started":"2025-04-16T13:18:05.093442Z","shell.execute_reply":"2025-04-16T13:21:03.942963Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nCollecting torchtext\n  Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext) (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.32.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:08\u001b[0mmm\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchtext\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchtext-0.18.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Load and Show the dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# File paths\ntwitter_path = \"/kaggle/input/twitter-and-reddit-sentimental-analysis-dataset/Twitter_Data.csv\"\nreddit_path = \"/kaggle/input/twitter-and-reddit-sentimental-analysis-dataset/Reddit_Data.csv\"\n\n# Load datasets\ndf_twitter = pd.read_csv(twitter_path)\n# .drop(columns=['clean_comment'])\ndf_reddit = pd.read_csv(reddit_path).rename(columns={'clean_comment' : 'clean_text'})\n# .drop(columns=['clean_comment'])\n\n# Display shapes\nprint(f\"Twitter Data shape: {df_twitter.shape}\")\nprint(f\"Reddit Data shape: {df_reddit.shape}\")\n\n# Concatenate datasets\ndf = pd.concat([df_twitter, df_reddit], axis=0).reset_index(drop=True)\n\n# Display final shape and preview\nprint(f\"Combined Data shape: {df.shape}\")\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:21:03.944521Z","iopub.execute_input":"2025-04-16T13:21:03.944729Z","iopub.status.idle":"2025-04-16T13:21:04.818914Z","shell.execute_reply.started":"2025-04-16T13:21:03.944704Z","shell.execute_reply":"2025-04-16T13:21:04.818237Z"}},"outputs":[{"name":"stdout","text":"Twitter Data shape: (162980, 2)\nReddit Data shape: (37249, 2)\nCombined Data shape: (200229, 2)\n                                          clean_text  category\n0  when modi promised “minimum government maximum...      -1.0\n1  talk all the nonsense and continue all the dra...       0.0\n2  what did just say vote for modi  welcome bjp t...       1.0\n3  asking his supporters prefix chowkidar their n...       1.0\n4  answer who among these the most powerful world...       1.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"df_twitter.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:21:04.820669Z","iopub.execute_input":"2025-04-16T13:21:04.820922Z","iopub.status.idle":"2025-04-16T13:21:04.827624Z","shell.execute_reply.started":"2025-04-16T13:21:04.820901Z","shell.execute_reply":"2025-04-16T13:21:04.826993Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Index(['clean_text', 'category'], dtype='object')"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"df_reddit.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:21:04.828349Z","iopub.execute_input":"2025-04-16T13:21:04.828576Z","iopub.status.idle":"2025-04-16T13:21:04.842351Z","shell.execute_reply.started":"2025-04-16T13:21:04.828557Z","shell.execute_reply":"2025-04-16T13:21:04.841633Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Index(['clean_text', 'category'], dtype='object')"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"### Perform Preprocessing Task","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom collections import Counter\nimport pandas as pd\nimport nltk\nnltk.download('punkt')\n\n# Load dataset\n# df = pd.read_csv(\"Twitter_Data.csv\")  # Replace with your dataset path\ndf = df.dropna()\ndf['label'] = df['category'].astype('category').cat.codes\n\n# Custom tokenizer using nltk\ndef tokenizer(text):\n    return nltk.word_tokenize(text.lower())\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(df['clean_text'], df['label'], test_size=0.2, random_state=42)\n\n# Build vocabulary manually\ndef build_vocab(texts, min_freq=1):\n    counter = Counter()\n    for text in texts:\n        counter.update(tokenizer(text))\n    vocab = {\"<unk>\": 0, \"<pad>\": 1}\n    for word, freq in counter.items():\n        if freq >= min_freq:\n            vocab[word] = len(vocab)\n    return vocab\n\nprint(\"Building vocabulary...\")\nvocab = build_vocab(X_train)\nprint(f\"Vocabulary size: {len(vocab)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:21:04.843340Z","iopub.execute_input":"2025-04-16T13:21:04.843609Z","iopub.status.idle":"2025-04-16T13:21:26.696723Z","shell.execute_reply.started":"2025-04-16T13:21:04.843585Z","shell.execute_reply":"2025-04-16T13:21:26.696073Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Building vocabulary...\nVocabulary size: 116684\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Getting the data ready & Perform the training","metadata":{}},{"cell_type":"code","source":"# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, vocab, tokenizer, max_len=100):\n        self.texts = texts\n        self.labels = labels\n        self.vocab = vocab\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        tokens = self.tokenizer(self.texts[idx])\n        ids = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens][:self.max_len]\n        ids += [self.vocab[\"<pad>\"]] * (self.max_len - len(ids))\n        return torch.tensor(ids), torch.tensor(self.labels[idx])\n\n# DataLoader\nprint(\"Preparing DataLoaders...\")\ntrain_dataset = TextDataset(X_train.tolist(), y_train.tolist(), vocab, tokenizer)\ntest_dataset = TextDataset(X_test.tolist(), y_test.tolist(), vocab, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n# LSTM Model\nclass LSTMModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n        super(LSTMModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[\"<pad>\"])\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        _, (hidden, _) = self.lstm(embedded)\n        return self.fc(hidden[-1])\n\n# Model setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvocab_size = len(vocab)\nembed_dim = 100\nhidden_dim = 128\noutput_dim = y_train.nunique()\n\nmodel = LSTMModel(vocab_size, embed_dim, hidden_dim, output_dim).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nprint(\"Starting training...\\n\")\n# Training loop\nfor epoch in range(1, 6):\n    model.train()\n    total_loss = 0\n    batch_count = 0\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        batch_count += 1\n        if batch_count % 1000 == 0:\n            print(f\"Epoch {epoch} | Batch {batch_count} | Batch Loss: {loss.item():.4f}\")\n    print(f\"Epoch {epoch} Completed | Avg Loss: {total_loss / batch_count:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:21:26.697584Z","iopub.execute_input":"2025-04-16T13:21:26.698024Z","iopub.status.idle":"2025-04-16T13:25:53.878559Z","shell.execute_reply.started":"2025-04-16T13:21:26.698002Z","shell.execute_reply":"2025-04-16T13:25:53.877871Z"}},"outputs":[{"name":"stdout","text":"Preparing DataLoaders...\nStarting training...\n\nEpoch 1 | Batch 1000 | Batch Loss: 1.0729\nEpoch 1 | Batch 2000 | Batch Loss: 0.6141\nEpoch 1 | Batch 3000 | Batch Loss: 0.5082\nEpoch 1 | Batch 4000 | Batch Loss: 0.3578\nEpoch 1 | Batch 5000 | Batch Loss: 0.1966\nEpoch 1 Completed | Avg Loss: 0.6370\n\nEpoch 2 | Batch 1000 | Batch Loss: 0.5426\nEpoch 2 | Batch 2000 | Batch Loss: 0.4038\nEpoch 2 | Batch 3000 | Batch Loss: 0.1507\nEpoch 2 | Batch 4000 | Batch Loss: 0.4252\nEpoch 2 | Batch 5000 | Batch Loss: 0.1272\nEpoch 2 Completed | Avg Loss: 0.2367\n\nEpoch 3 | Batch 1000 | Batch Loss: 0.0245\nEpoch 3 | Batch 2000 | Batch Loss: 0.0443\nEpoch 3 | Batch 3000 | Batch Loss: 0.2106\nEpoch 3 | Batch 4000 | Batch Loss: 0.0350\nEpoch 3 | Batch 5000 | Batch Loss: 0.2007\nEpoch 3 Completed | Avg Loss: 0.1492\n\nEpoch 4 | Batch 1000 | Batch Loss: 0.0431\nEpoch 4 | Batch 2000 | Batch Loss: 0.0726\nEpoch 4 | Batch 3000 | Batch Loss: 0.0529\nEpoch 4 | Batch 4000 | Batch Loss: 0.1123\nEpoch 4 | Batch 5000 | Batch Loss: 0.1178\nEpoch 4 Completed | Avg Loss: 0.1019\n\nEpoch 5 | Batch 1000 | Batch Loss: 0.0261\nEpoch 5 | Batch 2000 | Batch Loss: 0.0109\nEpoch 5 | Batch 3000 | Batch Loss: 0.0948\nEpoch 5 | Batch 4000 | Batch Loss: 0.0850\nEpoch 5 | Batch 5000 | Batch Loss: 0.0110\nEpoch 5 Completed | Avg Loss: 0.0712\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Perform Evaluation on Trained Model","metadata":{}},{"cell_type":"code","source":"# Evaluation\nprint(\"Starting evaluation...\\n\")\nmodel.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for i, (X_batch, y_batch) in enumerate(test_loader):\n        X_batch = X_batch.to(device)\n        output = model(X_batch)\n        preds = torch.argmax(output, dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(y_batch.numpy())\n        if i % 120 == 0:\n            print(f\"Processed batch {i+1} in test set\")\n\naccuracy = accuracy_score(all_labels, all_preds)\nprint(f\"\\nTest Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:25:53.879329Z","iopub.execute_input":"2025-04-16T13:25:53.879748Z","iopub.status.idle":"2025-04-16T13:26:01.342624Z","shell.execute_reply.started":"2025-04-16T13:25:53.879715Z","shell.execute_reply":"2025-04-16T13:26:01.341981Z"}},"outputs":[{"name":"stdout","text":"Starting evaluation...\n\nProcessed batch 1 in test set\nProcessed batch 121 in test set\nProcessed batch 241 in test set\nProcessed batch 361 in test set\nProcessed batch 481 in test set\nProcessed batch 601 in test set\nProcessed batch 721 in test set\nProcessed batch 841 in test set\nProcessed batch 961 in test set\nProcessed batch 1081 in test set\nProcessed batch 1201 in test set\n\nTest Accuracy: 0.9574\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Save the model","metadata":{}},{"cell_type":"code","source":"# Save model state\nmodel_path = \"/kaggle/working/lstm_sentiment_model.pth\"\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'vocab': vocab,  # optional: save vocab for inference\n    'embed_dim': embed_dim,\n    'hidden_dim': hidden_dim,\n    'output_dim': output_dim\n}, model_path)\n\nprint(f\"Model saved to {model_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:26:01.343375Z","iopub.execute_input":"2025-04-16T13:26:01.343651Z","iopub.status.idle":"2025-04-16T13:26:01.550281Z","shell.execute_reply.started":"2025-04-16T13:26:01.343631Z","shell.execute_reply":"2025-04-16T13:26:01.549490Z"}},"outputs":[{"name":"stdout","text":"Model saved to /kaggle/working/lstm_sentiment_model.pth\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Load the saved model and evaluate","metadata":{}},{"cell_type":"code","source":"# Load model state\ncheckpoint = torch.load(\"/kaggle/working/lstm_sentiment_model.pth\", map_location=device)\n\n# Recreate model architecture\nmodel1 = LSTMModel(len(vocab), checkpoint['embed_dim'], checkpoint['hidden_dim'], checkpoint['output_dim'])\nmodel1.load_state_dict(checkpoint['model_state_dict'])\nmodel1.to(device)\nmodel1.eval()\n\nprint(\"Model loaded and ready for inference.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:27:37.463884Z","iopub.execute_input":"2025-04-16T13:27:37.464478Z","iopub.status.idle":"2025-04-16T13:27:37.732916Z","shell.execute_reply.started":"2025-04-16T13:27:37.464454Z","shell.execute_reply":"2025-04-16T13:27:37.732184Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/828793302.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(\"/kaggle/working/lstm_sentiment_model.pth\", map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Model loaded and ready for inference.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score\n\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        outputs = model1(X_batch)\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(y_batch.cpu().numpy())\n\n# Classification report\nprint(\"Evaluation Report:\\n\")\nprint(classification_report(all_labels, all_preds))\n\n# Accuracy\nacc = accuracy_score(all_labels, all_preds)\nprint(f\"Accuracy: {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:29:08.886899Z","iopub.execute_input":"2025-04-16T13:29:08.887212Z","iopub.status.idle":"2025-04-16T13:29:16.642086Z","shell.execute_reply.started":"2025-04-16T13:29:08.887191Z","shell.execute_reply":"2025-04-16T13:29:16.641301Z"}},"outputs":[{"name":"stdout","text":"Evaluation Report:\n\n              precision    recall  f1-score   support\n\n           0       0.90      0.93      0.91      8615\n           1       0.98      0.97      0.98     13630\n           2       0.97      0.96      0.96     17779\n\n    accuracy                           0.96     40024\n   macro avg       0.95      0.95      0.95     40024\nweighted avg       0.96      0.96      0.96     40024\n\nAccuracy: 0.9574\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}